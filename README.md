# ğŸ¤ vibrato: VIterbi-Based acceleRAted TOkenizer

## Resource preparation

```
$ ./scripts/prepare_ipadic-mecab-2_7_0.sh
$ ./scripts/prepare_unidic-cwj-3_1_0.sh
$ ./scripts/prepare_unidic-mecab-2_1_2.sh
```

## Compiling dictionary

```
$ cargo run --release -p compile -- -r resources_ipadic-mecab-2_7_0 -o system.dic
Compiling the system dictionary...
1.593941214 seconds
Writting the system dictionary...: system.dic
44.63689613342285 MiB
```

## Do mapping

```
$ cargo run --release -p compile --bin map -- -i system.dic -t data/wagahaiwa_nekodearu.txt -o system.mapped.dic
Loading the dictionary...
Training connection id mappings...
Writting the system dictionary...: system.mapped.dic
44.642452239990234 MiB
```

## Compiling user lexicon

```
$ cargo run --release -p compile --bin user -- -i system.dic -u data/user_example.csv -o user.dic
Loading the system dictionary...
Compiling the user lexicon...
Writting the user dictionary...: user.dic
0.11874866485595703 MiB
```

## Tokenize

```
$ echo 'æœ¬ã¨ã‚«ãƒ¬ãƒ¼ã®è¡—ç¥ä¿ç”ºã¸ã‚ˆã†ã“ãã€‚' | cargo run --release -p tokenize -- -i system.dic
Loading the dictionary...
Ready to tokenize :)
æœ¬      åè©,ä¸€èˆ¬,*,*,*,*,æœ¬,ãƒ›ãƒ³,ãƒ›ãƒ³
ã¨      åŠ©è©,ä¸¦ç«‹åŠ©è©,*,*,*,*,ã¨,ãƒˆ,ãƒˆ
ã‚«ãƒ¬ãƒ¼  åè©,å›ºæœ‰åè©,åœ°åŸŸ,ä¸€èˆ¬,*,*,ã‚«ãƒ¬ãƒ¼,ã‚«ãƒ¬ãƒ¼,ã‚«ãƒ¬ãƒ¼
ã®      åŠ©è©,é€£ä½“åŒ–,*,*,*,*,ã®,ãƒ,ãƒ
è¡—      åè©,ä¸€èˆ¬,*,*,*,*,è¡—,ãƒãƒ,ãƒãƒ
ç¥ä¿    åè©,å›ºæœ‰åè©,åœ°åŸŸ,ä¸€èˆ¬,*,*,ç¥ä¿,ã‚¸ãƒ³ãƒœã‚¦,ã‚¸ãƒ³ãƒœãƒ¼
ç”º      åè©,æ¥å°¾,åœ°åŸŸ,*,*,*,ç”º,ãƒãƒ,ãƒãƒ
ã¸      åŠ©è©,æ ¼åŠ©è©,ä¸€èˆ¬,*,*,*,ã¸,ãƒ˜,ã‚¨
ã‚ˆã†ã“ã        æ„Ÿå‹•è©,*,*,*,*,*,ã‚ˆã†ã“ã,ãƒ¨ã‚¦ã‚³ã‚½,ãƒ¨ãƒ¼ã‚³ã‚½
ã€‚      è¨˜å·,å¥ç‚¹,*,*,*,*,ã€‚,ã€‚,ã€‚
EOS
```

```
$ echo 'æœ¬ã¨ã‚«ãƒ¬ãƒ¼ã®è¡—ç¥ä¿ç”ºã¸ã‚ˆã†ã“ãã€‚' | cargo run --release -p tokenize -- -i system.dic -m wakati
Loading the dictionary...
Ready to tokenize :)
æœ¬ ã¨ ã‚«ãƒ¬ãƒ¼ ã® è¡— ç¥ä¿ ç”º ã¸ ã‚ˆã†ã“ã ã€‚
```

```
$ cat data/user_example.csv
æœ¬ã¨ã‚«ãƒ¬ãƒ¼,1293,1293,0,ã‚«ã‚¹ã‚¿ãƒ åè©,ãƒ›ãƒ³ãƒˆã‚«ãƒ¬ãƒ¼
ç¥ä¿ç”º,1293,1293,334,ã‚«ã‚¹ã‚¿ãƒ åè©,ã‚¸ãƒ³ãƒœãƒãƒ§ã‚¦
ã‚ˆã†ã“ã,3,3,-1000,æ„Ÿå‹•è©,ãƒ¨ãƒ¼ã‚³ã‚½,Welcome,æ¬¢è¿æ¬¢è¿,Benvenuto,Willkommen
```

```
$ echo 'æœ¬ã¨ã‚«ãƒ¬ãƒ¼ã®è¡—ç¥ä¿ç”ºã¸ã‚ˆã†ã“ãã€‚' | cargo run --release -p tokenize -- -i system.dic -u user.dic
Loading the dictionary...
Ready to tokenize :)
æœ¬ã¨ã‚«ãƒ¬ãƒ¼	ã‚«ã‚¹ã‚¿ãƒ åè©,ãƒ›ãƒ³ãƒˆã‚«ãƒ¬ãƒ¼
ã®	åŠ©è©,é€£ä½“åŒ–,*,*,*,*,ã®,ãƒ,ãƒ
è¡—	åè©,ä¸€èˆ¬,*,*,*,*,è¡—,ãƒãƒ,ãƒãƒ
ç¥ä¿ç”º	ã‚«ã‚¹ã‚¿ãƒ åè©,ã‚¸ãƒ³ãƒœãƒãƒ§ã‚¦
ã¸	åŠ©è©,æ ¼åŠ©è©,ä¸€èˆ¬,*,*,*,ã¸,ãƒ˜,ã‚¨
ã‚ˆã†ã“ã	æ„Ÿå‹•è©,ãƒ¨ãƒ¼ã‚³ã‚½,Welcome,æ¬¢è¿æ¬¢è¿,Benvenuto,Willkommen
ã€‚	è¨˜å·,å¥ç‚¹,*,*,*,*,ã€‚,ã€‚,ã€‚
EOS
```

## Benchmark

```
$ cargo run --release -p benchmark -- -i system.dic < benchmark/data/wagahaiwa_nekodearu.txt
[benchmark/src/main.rs:50] n_words = 2462700
Warmup: 0.0813649
[benchmark/src/main.rs:50] n_words = 2462700
[benchmark/src/main.rs:50] n_words = 2462700
[benchmark/src/main.rs:50] n_words = 2462700
[benchmark/src/main.rs:50] n_words = 2462700
[benchmark/src/main.rs:50] n_words = 2462700
[benchmark/src/main.rs:50] n_words = 2462700
[benchmark/src/main.rs:50] n_words = 2462700
[benchmark/src/main.rs:50] n_words = 2462700
[benchmark/src/main.rs:50] n_words = 2462700
[benchmark/src/main.rs:50] n_words = 2462700
Number_of_sentences: 2376
Elapsed_seconds_to_tokenize_all_sentences: [0.07661468000000002,0.07816473125,0.08134009]
```
